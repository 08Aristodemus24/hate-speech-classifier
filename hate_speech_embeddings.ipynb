{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this notebook will accomplish the ff:\n",
    "- generate word embeddings from hate-speech-data-cleaned.csv\n",
    "- see words similar in context\n",
    "- save word embeddings in separate file for later use in training a sentiment classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utilities.data_preprocessors import read_preprocess, series_to_1D_array\n",
    "from utilities.data_visualizers import view_words, view_word_frequency\n",
    "\n",
    "import json\n",
    "import ast\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data\n",
    "recall that the the comments have designated labels which are 2, 1, and 0 representing derogatory, non-derogatory, and offensive comments respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[woman, complain, cleaning, house, man, always...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[boy, dat, coldtyga, dwn, bad, cuffin, dat, ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dawg, ever, fuck, bitch, start, cry, confused...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[look, like, tranny]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[shit, hear, might, true, might, faker, bitch,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65775</th>\n",
       "      <td>[from, the, midnight, sun, where, the, hot, sp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65776</th>\n",
       "      <td>[do, not, say, am, not, your, type]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65777</th>\n",
       "      <td>[and, therefor, never, send, to, know, for, wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65778</th>\n",
       "      <td>[and, cannot, stand, anoth, day]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65779</th>\n",
       "      <td>[all, valu, unless, otherwis, state, are, in, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65780 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  label\n",
       "0      [woman, complain, cleaning, house, man, always...      1\n",
       "1      [boy, dat, coldtyga, dwn, bad, cuffin, dat, ho...      0\n",
       "2      [dawg, ever, fuck, bitch, start, cry, confused...      0\n",
       "3                                   [look, like, tranny]      0\n",
       "4      [shit, hear, might, true, might, faker, bitch,...      0\n",
       "...                                                  ...    ...\n",
       "65775  [from, the, midnight, sun, where, the, hot, sp...      1\n",
       "65776                [do, not, say, am, not, your, type]      1\n",
       "65777  [and, therefor, never, send, to, know, for, wh...      1\n",
       "65778                   [and, cannot, stand, anoth, day]      1\n",
       "65779  [all, valu, unless, otherwis, state, are, in, ...      1\n",
       "\n",
       "[65780 rows x 2 columns]"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/hate-speech-data-cleaned.csv', index_col=0)\n",
    "df = read_preprocess(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract diagnosis as Y then\n",
    "# transform Y to 2-dim 1 x m matrix\n",
    "Y = df['label']\n",
    "Y = Y.to_numpy().reshape(Y.shape[0], -1)\n",
    "\n",
    "# extract comment column\n",
    "X = df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman', 'complain', 'cleaning', 'house', 'man', 'always', 'take', 'trash']"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0]\n",
    "# X[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model architecture and initialization\n",
    "- here the window or window size is the amount of words to use as context and target are indicated, as well as the min_count which indicates if a word length lower than its value is still to be considered part of the window, vector_size is the number of features each resulting word embedding would have, and workers which represent the number of threads to use in training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(window=5, min_count=2, vector_size=300, workers=4)\n",
    "model.build_vocab(X, progress_per=1000)\n",
    "print(model.corpus_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X, total_examples=model.corpus_count, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number (39226987) indicates the total number of processed words during training, while the second number (44743900) represents the total number of words in the corpus that the model has seen so far."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save words and their respective vectors to dictionary then .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vectors = model.wv.key_to_index, model.wv.vectors\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = dict(zip(vocab.keys(), vectors.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/word_vec.json', 'w') as out_file:\n",
    "    json.dump(word_vec, out_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploratory data analysis for the generated embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the vectorized version of a word in this case 'fuck'\n",
    "model.wv['fuck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the top 10 positively similar words\n",
    "# to the given word in this case 'fuck'\n",
    "model.wv.most_similar(positive=['fuck'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now in this case generate the most\n",
    "# negatively similar words to 'fuck'\n",
    "# or in other words the opposite of 'fuck'\n",
    "model.wv.most_similar(negative=['fuck'], topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the cosine similarity of the same word or perhaps words that are almost the same will be always 1 or approximating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the cosine similarity between two words or in other\n",
    "# words the measurement of how closely related words are\n",
    "model.wv.similarity('fuck', 'fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('fuck', 'faggot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('fuck', 'fucking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_words(word_vec, word_range=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate-speech-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
