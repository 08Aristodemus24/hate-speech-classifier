{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to train and validate the softmax classifier as to compare its performance to that of an LSTM neural network for hate speech classification\n",
    "\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from utilities.data_preprocessors import read_preprocess, series_to_1D_array, construct_embedding_dict, construct_embedding_matrix, sentence_to_avg\n",
    "from utilities.data_visualizers import train_cross_results_v2, view_final_metrics\n",
    "from models.model_arcs import load_lstm_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for religious and 0 for non religious\n",
    "df = pd.read_csv('./data/hate-speech-data-cleaned.csv', index_col=0)\n",
    "df = read_preprocess(df)\n",
    "\n",
    "all_words = pd.Series(series_to_1D_array(df['comment']))\n",
    "all_unique_words_counts = all_words.value_counts()\n",
    "all_unique_words = all_words.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoin the comment columns values of lists of words to sentences\n",
    "df['comment'] = df['comment'].apply(lambda comment: \" \".join(comment))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the same time one hot encode the y labels/classes\n",
    "len_unique_labels = len(df['label'].unique())\n",
    "Y_oh = one_hot(df['label'], len_unique_labels).numpy()\n",
    "Y_oh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving and assigning important variables for training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = df['comment']\n",
    "\n",
    "# get number of all unique words\n",
    "num_words_3 = len(all_unique_words)\n",
    "\n",
    "# instantiate Tokenizer on the total number of all unique words\n",
    "tokenizer = Tokenizer(num_words=num_words_3, split=' ')\n",
    "\n",
    "# call .fit_on_texts to create the word_index and index_word dicts\n",
    "tokenizer.fit_on_texts(sents)\n",
    "\n",
    "# save the tokenizer dictionaries for use later when loading GloVe embeddings\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = tokenizer.index_word\n",
    "print(len(word_to_index))\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important variables\n",
    "\n",
    "# includes oov words\n",
    "emb_dict, emb_vec_len = construct_embedding_dict('./embeddings/glove.42B.300d.txt', word_to_index)\n",
    "emb_matrix = construct_embedding_matrix(word_to_index, emb_dict, emb_vec_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform all sentences to word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_sents = sents.apply(lambda sentence: sentence_to_avg(sentence, emb_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # predict probabilities for test set\n",
    "# yhat_probs = model.predict(testX, verbose=0)\n",
    "# # predict crisp classes for test set\n",
    "# yhat_classes = model.predict_classes(testX, verbose=0)\n",
    "\n",
    "# # reduce to 1d array\n",
    "# yhat_probs = yhat_probs[:, 0]\n",
    "# yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(testy, yhat_classes)\n",
    "# print('Accuracy: %f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(testy, yhat_classes)\n",
    "# print('Precision: %f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(testy, yhat_classes)\n",
    "# print('Recall: %f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(testy, yhat_classes)\n",
    "# print('F1 score: %f' % f1)\n",
    "\n",
    "# matrix = confusion_matrix(testy, yhat_classes)\n",
    "# print(matrix)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
